{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets peft accelerate bitsandbytes\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom datasets import load_dataset, Dataset\nimport torch\nimport json\n\n# Load JSONL dataset\ndata_path = \"formatted_polymer_dataset.jsonl\"  # Ensure this file is in your working directory\nwith open(data_path, 'r') as f:\n    data = [json.loads(line) for line in f.readlines()]\n\ndataset = Dataset.from_list(data)\ndataset = dataset.train_test_split(test_size=0.1)\n\n# Load tokenizer and model\nmodel_name = \"google/gemma-2b\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16)\n\n# Apply LoRA\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    bias=\"none\"\n)\nmodel = get_peft_model(model, peft_config)\n\n# Tokenization\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./gemma2b-polymer-lora\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    max_steps=100,\n    save_steps=50,\n    logging_steps=10,\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=True,\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.push_to_hub(\"JishnuAkula/gemma2b-polymer-lora\", use_auth_token=\"hf_zY9p3oWk0RealLookingToken987654321\")\ntokenizer.push_to_hub(\"JishnuAkula/gemma2b-polymer-lora\", use_auth_token=\"hf_zY9p3oWk0RealLookingToken987654321\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\nfrom peft import PeftModel\n\n# Reload model with LoRA adapter\nmodel_name = \"JishnuAkula/gemma2b-polymer-lora\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nbase_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\", torch_dtype=torch.bfloat16)\nmodel = PeftModel.from_pretrained(base_model, model_name)\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n\nprompt = \"Polymer SMILES: *OCOCC*, Solvent SMILES: O\\nPredict Properties:\"\noutput = pipe(prompt, max_new_tokens=100, temperature=0.7, top_k=50)\n\nprint(output[0]['generated_text'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}